---
title: LFM降维
date: 2021-02-08 15:59:41
categories: 
- 理论归纳
tags:
- 降维分析
mathjax: true
---

# LFM（Latent Factor Model）

​	LFM有监督的降维，其思想是尽可能使偷饮后同类型数据的方差尽可能小，不同类型数据距离近可能大。

## 矩阵因子分解

​	假设存在$m\times n$的矩阵R，假如想要发现k个隐类（隐藏特征），则需要找到两个矩阵P(m特征矩阵)和Q（n调整矩阵），使这两个矩阵乘积等于R，即将R分解为两个低纬矩阵相乘。
$$
\widehat{R}_{m\times n}=P_{m\times k}^T · Q_{k\times n} \approx R
$$

## 预测位置数值计算

​	假如$R_{m\times n}$代表m个用户对n个物品的评分矩阵它是一个稀疏矩阵，则$P_{m\times k}$代表用户受k个隐藏特征的影响，$Q_{k\times n}$。代表物品受这k个隐藏特征的影响。

所以以u代表某个用户，以i代表某个物品，预测用户u对i的评分。
$$
r_{ui} = P_u^T·Q_i=\sum_{k=1}^KP_{uk}·Q_{ki}
$$
​	最终通过每一个$r_{ui}$计算得到预测评分矩阵$\widehat R$就不是稀疏矩阵了，而如果$\widehat R$在对应位置的值与R相近则认为预测位置的值也是近似的。

## 损失函数

​	继续上面的例子矩阵分解得到的预测评分矩阵$\widehat R$与原评分矩阵R在已知的评分项上可能存在误差，需要找到一个最优的分解方式，让分解后的预测评分矩阵总误差最小。这是可以使用损失函数

- 平方损失函数

  使用平方损失函数加入正则化项可以防止过拟合

  每一个预测评分都和原始评分做一个差值，之后平方求和后加正则化项（对$P_u$和$Q_i$做惩罚因为隐藏特征k越大PQ就会越复杂，越拟合）
  $$
  C = \sum _{(u,i)\in R_o}(R_{ui}-\widehat R_{ui})^2 + Reg \\
    = \sum_{(u,i)\in R_o}(R_{ui}-P_u^T·Q_i)^2 + \lambda \sum_u ||P_u||^2+\lambda \sum_i||Q_i||^2
  $$
  其中$\lambda \sum_u ||P_u||^2+\lambda \sum_i||Q_i||^2$是正则化项，$\lambda$一般通过交叉验证得到